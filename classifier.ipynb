{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split,  cross_val_score, cross_validate, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, recall_score, ConfusionMatrixDisplay\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "from typing import Tuple\n",
    "\n",
    "DIGITS=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset() -> np.ndarray:\n",
    "    dataset = load_iris()\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def split_dataset(dataset) -> np.ndarray:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(dataset['data'], dataset['target'], test_size = 0.2)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# region X\n",
    "# def split_data(feature_data: np.ndarray, target_data: np.ndarray, test_data_rate: float) ->  np.ndarray:\n",
    "#     if feature_data is None:\n",
    "#         raise Exception('Error : no input feature data')\n",
    "\n",
    "#     if test_data_rate is None:\n",
    "#         raise Exception('Error : no input test data rate')\n",
    "\n",
    "#     if test_data_rate >= 1 or test_data_rate <= 0:\n",
    "#         raise Exception('Error : test data rate must be a float value between 0 and 1')\n",
    "\n",
    "#     train_data, test_data, train_target, test_target = train_test_split(feature_data, target_data,   \n",
    "#     test_size=test_data_rate, shuffle=True)\n",
    "\n",
    "#     return train_data, test_data, train_target, test_target\n",
    "# endregion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_name(dataset) -> np.ndarray:\n",
    "    return dataset.feature_names\n",
    "\n",
    "def get_targets_name(dataset) -> np.ndarray:\n",
    "    return dataset.target_names\n",
    "\n",
    "def get_min_max_feature_data(data:np.ndarray) -> np.ndarray:\n",
    "    min_values = np.min(data, axis=0)\n",
    "    max_values = np.max(data, axis=0)\n",
    "\n",
    "    return min_values, max_values\n",
    "    \n",
    "def print_min_max_feature_data(data: np.ndarray, features: np.ndarray):\n",
    "    number_of_feature = data.shape[1]    \n",
    "\n",
    "    for i in range(0, number_of_feature):\n",
    "        min_value = round(min(data[:, i]), DIGITS)\n",
    "        max_value = round(max(data[:, i]), DIGITS)\n",
    "\n",
    "        print(\"feature(\" + features[i] + \")'s range: \" + str(min_value) + ' to ' + str(max_value)) \n",
    "\n",
    "def visualizing_dataset(dataset):\n",
    "    col_names = list(dataset.feature_names)\n",
    "    col_names.append('target')\n",
    "\n",
    "    df = pd.DataFrame(np.c_[dataset.data, dataset.target], columns=col_names)\n",
    "    display(df)\n",
    "\n",
    "    sns.pairplot(df, hue='target', vars=dataset.feature_names)    \n",
    "    \n",
    "    # plt.figure(figsize=(20,10))\n",
    "    # sns.heatmap(df.corr(), annot=True,square=True,cmap='RdBu')\n",
    "    # annot=True 데이터 값 셀 안에 적어줌, square=True 정사각형으로 출력    \n",
    "    # Color Brewer 팔레트 중 diverging : Diverging 팔레트는 양쪽으로 강조가 되기 때문에 낮은 값과 높은 값에 모두 관심을 가져야 하는 데이터 세트에 적합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_scaler_train(train_data: np.ndarray) ->  np.ndarray:\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    train_std = scaler.fit_transform(train_data)\n",
    "    # print_min_max_feature_data(X_train_std)\n",
    "\n",
    "    return train_std\n",
    "\n",
    "def set_scaler_train_test(train_data: np.ndarray, test_data: np.ndarray) ->  np.ndarray:\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # scaler.fit(X_train)\n",
    "    train_std = scaler.fit_transform(train_data)\n",
    "    test_data_std = scaler.transform(test_data)\n",
    "\n",
    "    # print_min_max_feature_data(X_train_std)\n",
    "    return train_std, test_data_std\n",
    "\n",
    "def set_scaler_train_validation_test(train_data: np.ndarray, validation_data: np.ndarray, test_data: np.ndarray) ->  np.ndarray:\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # scaler.fit(X_train)\n",
    "    train_std = scaler.fit_transform(train_data)\n",
    "    validation_std = scaler.transform(validation_data)    \n",
    "    test_data_std = scaler.transform(test_data)\n",
    "\n",
    "    # print_min_max_feature_data(X_train_std)\n",
    "    return train_std, validation_std, test_data_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set classification report parameter value\n",
    "# TARGET_NAMES=['class 0', 'class 1', 'class 2']\n",
    "# TARGET_NAMES=get_targets_name()\n",
    "\n",
    "def get_confusion_matric(test_target: np.ndarray, predicted_target: np.ndarray) -> np.ndarray:\n",
    "    return confusion_matrix(test_target, predicted_target)\n",
    "\n",
    "def cal_confusion_matric(test_target: np.ndarray, predicted_target: np.ndarray, targets_name: np.ndarray) -> np.ndarray:  \n",
    "    matric = confusion_matrix(test_target, predicted_target) # , labels=[0, 1, 2]\n",
    "    \n",
    "    report = classification_report(test_target, predicted_target, target_names=targets_name, digits=DIGITS)    \n",
    "\n",
    "    ACC = accuracy_score(test_target, predicted_target)\n",
    "    \n",
    "    FP = matric.sum(axis=0) - np.diag(matric) # 세로에서 TP 뺴기    \n",
    "    FN = matric.sum(axis=1) - np.diag(matric) # 가로에서 TP 빼기    \n",
    "    TP = np.diag(matric)    \n",
    "    TN = matric.sum() - (FP + FN + TP)\n",
    "    \n",
    "    FP = FP.astype(float)\n",
    "    FN = FN.astype(float)\n",
    "    TP = TP.astype(float)\n",
    "    TN = TN.astype(float)\n",
    "\n",
    "    # Sensitivity, hit rate, recall, or true positive rate\n",
    "    TPR = TP/(TP+FN) # TPR = recall_score(y_test, predicted_target, average=None or 'macro)    \n",
    "     # False negative rate\n",
    "    FNR = FN/(TP+FN) # FNR = 1 - TPR    \n",
    "    # Specificity or true negative rate\n",
    "    TNR = TN/(TN+FP)     \n",
    "    # Fall out or false positive rate\n",
    "    FPR = FP/(FP+TN) # FPR = 1 - TNR\n",
    "    \n",
    "    # region X\n",
    "    # Precision or positive predictive value\n",
    "    # PPV = TP/(TP+FP)\n",
    "    # Negative predictive value\n",
    "    # NPV = TN/(TN+FN)\n",
    "    # False discovery rate\n",
    "    # FDR = FP/(TP+FP)\n",
    "    # # Overall accuracy\n",
    "    # ACC = (TP+TN)/(TP+FP+FN+TN) \n",
    "\n",
    "    # print(\"TPR :\", TPR)\n",
    "    # print(\"FNR :\", FNR)\n",
    "    # print(\"TNR :\", TNR)\n",
    "    # print(\"FPR :\", FPR)\n",
    "    # endregion\n",
    "\n",
    "    return matric, report, ACC, TPR, FNR, TNR, FPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set classifier parameter value\n",
    "C=10\n",
    "MAX_ITER=10000\n",
    "GAMMA='auto' # rbf, poly, sigmoid\n",
    "DEGREE=3 # poly\n",
    "\n",
    "def set_svm_model(kernel: str):\n",
    "    if kernel == \"linear\":\n",
    "        svm_model = svm.SVC(kernel=\"linear\", C=C, max_iter=MAX_ITER)\n",
    "    elif kernel == \"rbf\":\n",
    "        svm_model = svm.SVC(kernel='rbf', C=C, gamma=GAMMA, max_iter=MAX_ITER)\n",
    "    elif kernel == \"poly\":\n",
    "        svm_model = svm.SVC(kernel='poly', C=C, gamma=GAMMA, degree=DEGREE, max_iter=MAX_ITER)  \n",
    "    elif kernel == \"sigmoid\":\n",
    "        svm_model = svm.SVC(kernel='sigmoid', C=C, gamma=GAMMA, max_iter=MAX_ITER)  \n",
    "    elif kernel == \"LinearSVC\":\n",
    "        svm_model = svm.LinearSVC(C=C, max_iter=MAX_ITER)\n",
    "    \n",
    "    return svm_model\n",
    "\n",
    "def set_fit_svm_model(kernel: str, train_data_std: np.ndarray, train_target: np.ndarray):\n",
    "    svm_model = set_svm_model(kernel)\n",
    "    \n",
    "    return svm_model.fit(train_data_std, train_target)\n",
    "\n",
    "def predict_svm_classification(svm_model, test_data_std: np.ndarray, test_target: np.ndarray, targets_name: np.ndarray, now_date_time):        \n",
    "    predicted_target = svm_model.predict(test_data_std)\n",
    "    # print(\"정답 라벨:\", test_target)\n",
    "    # print(\"예측 라벨:\", predicted_target)   \n",
    "        \n",
    "    cfm, report, acc, tpr, fnr, tnr, fpr = cal_confusion_matric(test_target, predicted_target, targets_name)\n",
    "\n",
    "    acc = np.round(acc, DIGITS)\n",
    "    tpr = np.round(tpr, DIGITS)\n",
    "    fnr = np.round(fnr, DIGITS)\n",
    "    tnr = np.round(tnr, DIGITS)\n",
    "    fpr = np.round(fpr, DIGITS)\n",
    "    \n",
    "    if hasattr(svm_model, 'kernel') == True:\n",
    "        print(\"\\nkernel(\" + svm_model.kernel + \")'s Confusion metric:\")\n",
    "        cfm = get_confusion_matric(test_target, predicted_target)        \n",
    "        disp_cfm = ConfusionMatrixDisplay.from_estimator(svm_model, test_data_std, test_target, display_labels=targets_name, cmap=plt.cm.Blues, normalize=None)\n",
    "        disp_cfm.ax_.set_title(\"Confusion matrix\")\n",
    "        # plt.figure()        \n",
    "        plt.savefig('results/' + 'Confusion matrix (' + svm_model.kernel + ') ' + now_date_time + '.jpg')\n",
    "        plt.show()\n",
    "\n",
    "        print(\"kernel(\" + svm_model.kernel + \")'s Classification report:\\n\", report)\n",
    "        print(\"kernel(\" + svm_model.kernel + \")'s Accuracy:\", acc)    \n",
    "        # print(\"kernel(\" + kernel + \")'s prediction accuracy: {:.2f}\".format(np.mean(predicted_target == test_target)))    \n",
    "        # print(\"kernel(\" + svm_model.kernel + \")'s TPR: \", tpr)\n",
    "        # print(\"kernel(\" + svm_model.kernel + \")'s FNR: \", fnr)\n",
    "        # print(\"kernel(\" + svm_model.kernel + \")'s TNR: \", tnr)\n",
    "        # print(\"kernel(\" + svm_model.kernel + \")'s FPR: \", fpr)   \n",
    "    else:\n",
    "        print(\"\\nLinearSVC's Confusion metric:\")\n",
    "        cfm = get_confusion_matric(test_target, predicted_target)\n",
    "        disp_cfm = ConfusionMatrixDisplay.from_estimator(svm_model, test_data_std, test_target, display_labels=targets_name, cmap=plt.cm.Blues, normalize=None)\n",
    "        disp_cfm.ax_.set_title(\"Confusion matrix\")\n",
    "        # plt.figure()        \n",
    "        plt.savefig('results/Confusion matrix (LinearSVC) ' + now_date_time + '.jpg')\n",
    "        plt.show()\n",
    "\n",
    "        print(\"LinearSVC's Classification report:\\n\", report)\n",
    "        print(\"LinearSVC's Accuracy:\", acc) \n",
    "        # print(\"LinearSVC's TPR:\", tpr)\n",
    "        # print(\"LinearSVC's FNR:\", fnr)\n",
    "        # print(\"LinearSVC's TNR:\", tnr)\n",
    "        # print(\"LinearSVC's FPR:\", fpr)    \n",
    "   \n",
    "    return acc, tpr, fnr, tnr, fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation \n",
    "def cross_validation_svm(kernel: str, cv: int, train_test_data: np.ndarray, train_test_target: np.ndarray, standardscaler: bool):\n",
    "    if standardscaler == True:\n",
    "        scaler = StandardScaler()\n",
    "        svm_model = set_svm_model(kernel)\n",
    "        pipeline = Pipeline([('scaler', scaler), ('model', svm_model)])\n",
    "\n",
    "        scores = cross_val_score(pipeline, train_test_data, train_test_target, cv=cv)\n",
    "    else:\n",
    "        svm_model = set_svm_model(kernel)\n",
    "\n",
    "        scores = cross_val_score(svm_model, train_test_data, train_test_target, cv=cv) # scoring='accuracy'\n",
    "    \n",
    "    if hasattr(svm_model, 'kernel') == True:\n",
    "        print(\"\\nkernel(\" + kernel + \")'s Cross-validation report:\\n\", pd.DataFrame(cross_validate(svm_model, train_test_data, train_test_target, cv=cv)))\n",
    "        print(\"kernel(\" + kernel + \")'s Cross-validation Accuracy:\", round(scores.mean(), DIGITS))\n",
    "    \n",
    "    else:\n",
    "        print(\"\\nLinearSVC's Cross-validation report:\\n\", pd.DataFrame(cross_validate(svm_model, train_test_data, train_test_target, cv=cv)))\n",
    "        print(\"LinearSVC's Cross-validation Accuracy:\", round(scores.mean(), DIGITS))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_nu = np.arange(0.001, 1.001, 0.001)\n",
    "# for nu in range(0, len(list_nu)):\n",
    "#     nu_value = list_nu[nu]\n",
    "#     gamma_value = 'auto'\n",
    "\n",
    "# parameters = {'C': list_nu}\n",
    "# PARAMETERS = {'C': [0.001, 0.01, 0.1, 1, 10, 25, 50, 100]}\n",
    "# PARAMETERS = {'C': [0.001, 0.01, 0.1, 1, 10, 25, 50, 100],\\\n",
    "#               'gamma':[0.001, 0.01, 0.1, 1, 10, 25, 50, 100]}\n",
    "\n",
    "# GridSearchCV\n",
    "def gridsearchCV_svm(kernel: str, cv: int, params: dict, train_test_data_std: np.ndarray, train_test_target: np.ndarray, standardscaler: bool):\n",
    "    if standardscaler == True:\n",
    "        scaler = StandardScaler()\n",
    "        svm_model = set_svm_model(kernel)\n",
    "        pipeline = Pipeline([('scaler', scaler), ('model', svm_model)])\n",
    "\n",
    "        grid_svm_model = GridSearchCV(pipeline, param_grid=params, cv=cv, verbose=1)\n",
    "        grid_svm_model.fit(train_test_data_std, train_test_target)\n",
    "\n",
    "    else:\n",
    "        svm_model = set_svm_model(kernel)\n",
    "    \n",
    "        grid_svm_model = GridSearchCV(svm_model, param_grid=params, cv=cv, verbose=1)\n",
    "        grid_svm_model.fit(train_test_data_std, train_test_target)\n",
    "    \n",
    "    result = pd.DataFrame(grid_svm_model.cv_results_['params'])\n",
    "    result['Accuracy'] = grid_svm_model.cv_results_['mean_test_score']\n",
    "    # result['mean_test_score'] = grid_svm_model.cv_results_['mean_test_score']    \n",
    "\n",
    "    if hasattr(svm_model, 'kernel') == True:\n",
    "        print(\"\\nkernel(\" + kernel + \")'s GridsearchCV Accuracy:\\n\", result.sort_values(by='Accuracy', ascending=False))\n",
    "        print(\"kernel(\" + kernel + \")'s GridsearchCV Best params:\", grid_svm_model.best_params_)\n",
    "\n",
    "    else:\n",
    "        print(\"\\nLinearSVC's GridsearchCV Accuracy:\\n\", result.sort_values(by='Accuracy', ascending=False))  \n",
    "        print(\"LinearSVC's GridsearchCV Best params:\", grid_svm_model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing SVM\n",
    "def visualizing_svm_prediction(kernel: str, features_name: list, targets_name: list, x_train_std: np.ndarray, y_train: np.ndarray):\n",
    "    plot_unit = 0.025\n",
    "    \n",
    "    for i in range(0, len(features_name), 2):\n",
    "        selected_train_std = x_train_std[:, i:i+2] # select ith and i+1th features        \n",
    "        model = set_fit_svm_model(kernel, selected_train_std, y_train)\n",
    "\n",
    "        feature_min_values, feature_max_values = get_min_max_feature_data(selected_train_std)\n",
    "\n",
    "        fir_feature_min, fir_feature_max = feature_min_values[0]-1, feature_max_values[0]+1\n",
    "        sec_feature_min, sec_feature_max = feature_min_values[1]-1, feature_max_values[1]+1\n",
    "\n",
    "        fir_feature, sec_feature = np.meshgrid(np.arange(fir_feature_min, fir_feature_max, plot_unit), np.arange(sec_feature_min, sec_feature_max, plot_unit))\n",
    "\n",
    "        predicted_target = model.predict(np.c_[fir_feature.ravel(), sec_feature.ravel()])\n",
    "        predicted_target = predicted_target.reshape(fir_feature.shape)\n",
    "\n",
    "        fir_target = selected_train_std[y_train==0] \n",
    "        sec_target = selected_train_std[y_train==1]\n",
    "        thd_target = selected_train_std[y_train==2]  \n",
    "                \n",
    "        plt.pcolormesh(fir_feature, sec_feature, predicted_target, alpha=0.1)        \n",
    "        # plt.scatter(selected_train_std[:, 0], selected_train_std[:, 1], c=y_train)        \n",
    "        plt.scatter(fir_target[:, 0], fir_target[:, 1])\n",
    "        plt.scatter(sec_target[:, 0], sec_target[:, 1])\n",
    "        plt.scatter(thd_target[:, 0], thd_target[:, 1])\n",
    "        plt.xlabel(features_name[i])\n",
    "        plt.ylabel(features_name[i+1])\n",
    "        plt.legend(targets_name, loc=4)\n",
    "        plt.xlim(fir_feature.min(), fir_feature.max())        \n",
    "        # plt.title('Support Vector Machine')\n",
    "        plt.show()    \n",
    "\n",
    "        # plt.pcolormesh(fir_feature, sec_feature, predicted_target, alpha=0.1)\n",
    "        # plt.scatter(selected_train_std[:, 0], selected_train_std[:, 1], c=y_train)        \n",
    "        # plt.xlabel(features_name[i])\n",
    "        # plt.ylabel(features_name[i+1])\n",
    "        # plt.legend(targets_name, loc=4)\n",
    "        # plt.xlim(fir_feature.min(), fir_feature.max())        \n",
    "        # # plt.title('Support Vector Machine')\n",
    "        # plt.show()   \n",
    "        \n",
    "def visualizing_svm_decision_region(kernel: str, features_name: list, targets_name:list, x_train_std:np.ndarray, y_train:np.ndarray):\n",
    "    for i in range(0, len(features_name), 2):\n",
    "        selected_train_std = x_train_std[:, i:i+2] # select ith and i+1th features        \n",
    "        model = set_fit_svm_model(kernel, selected_train_std, y_train)\n",
    "    \n",
    "        fir_target = selected_train_std[y_train==0] \n",
    "        sec_target = selected_train_std[y_train==1]\n",
    "        thd_target = selected_train_std[y_train==2]        \n",
    "\n",
    "        # plt.scatter(red_pt[:,0], red_pt[:,1], color='r')\n",
    "        plt.scatter(fir_target[:,0], fir_target[:,1])\n",
    "        plt.scatter(sec_target[:,0], sec_target[:,1])\n",
    "        plt.scatter(thd_target[:,0], thd_target[:,1])\n",
    "        plt.xlabel(features_name[i])\n",
    "        plt.ylabel(features_name[i+1])\n",
    "        plt.legend(targets_name, loc=4) \n",
    "        plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = load_dataset()\n",
    "\n",
    "# X_train, X_test, y_train, y_test = split_dataset(dataset)\n",
    "# X_train_std, X_test_std = set_scaler(X_train, X_test)\n",
    "\n",
    "# model = set_fit_svm_model(\"linear\", X_train_std, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16 (default, Jan 17 2023, 22:25:28) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
