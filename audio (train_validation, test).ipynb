{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "import IPython.display as ipd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import import_ipynb\n",
    "import preprocessor\n",
    "import display\n",
    "import classifier\n",
    "import csv_file_io\n",
    "import datetime\n",
    "\n",
    "PROJECT_HOME = './'\n",
    "# DRONE = 'Inspire2' # 'Bebop2', 'Spark', 'MavicPro', 'Phantom4', 'Matrice100', 'MavicAir', 'AutelEVO', 'Inspire2', 'JME', 'ParrotAnafi']\n",
    "# DATASET_DIR = PROJECT_HOME + 'dataset/validation/Inspire2/Inspire2_validation_File3.wav'\n",
    "RESULT_FILE_DIR = PROJECT_HOME + 'results/'\n",
    "DATASET_FILE_DIR = PROJECT_HOME + 'results/dataset/'\n",
    "# TRAINING_DATASET_DIR = PROJECT_HOME + 'dataset/1. training/'\n",
    "# VALIDATION_DATASET_DIR = PROJECT_HOME + 'dataset/2. validation/'\n",
    "# TEST_DATASET_DIR = PROJECT_HOME + 'dataset/3. test/'\n",
    "# TRAINING_VALIDATION_DATASET_DIR = PROJECT_HOME + 'dataset/4. training_validation/'\n",
    "# TRAINING_VALIDATION_TEST_DATASET_DIR = PROJECT_HOME + 'dataset/5. training_validation_test/'\n",
    "SEGMENTATION_DATASET_DIR = PROJECT_HOME + 'dataset/6. segmentation/'\n",
    "# SEGMENTATION_TEST_DATASET_DIR = PROJECT_HOME + 'dataset/7. segmentation(test)/'\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "now_date_time = now.strftime('%Y%m%d_%H%M%S')[2:]\n",
    "\n",
    "FILE_NAME = \"[Training, Validation, Test] Classification result ()\"\n",
    "\n",
    "SAMPLE_RATE = 44100\n",
    "FIG_SIZE = (15,5)\n",
    "\n",
    "FRAME_LENGTH = 0.250 # 250 ms\n",
    "FRAME_STRIDE = 0.250 # same with frame_length for 0% overlap\n",
    "\n",
    "DIGITS = 3\n",
    "\n",
    "b_visualizing = False\n",
    "\n",
    "def save_each_classification_performance(kernel: str, target: list, acc: float, tpr: float, fnr: float, tnr: float, fpr: float):\n",
    "    file_name = FILE_NAME.split(')')[0] + kernel + ') '\n",
    "\n",
    "    f = csv_file_io.open_csv(RESULT_FILE_DIR + file_name + now_date_time + '.csv', 'w')\n",
    "    csv_file_io.write_csv_data(f, ['Kernel', kernel])\n",
    "    csv_file_io.write_csv_data(f, ['ACC', acc])\n",
    "    csv_file_io.write_csv_data(f, ['', 'TPR', 'FNR', 'TNR', 'FPR'])\n",
    "    \n",
    "    for i in range (0, len(target)):\n",
    "        csv_file_io.write_csv_data(f, [target[i], tpr[i], fnr[i], tnr[i], fpr[i]])    \n",
    "\n",
    "    csv_file_io.write_csv_data(f, [])\n",
    "    csv_file_io.write_csv_data(f, ['Average', round(np.mean(tpr), DIGITS), round(np.mean(fnr), DIGITS), round(np.mean(tnr), DIGITS), round(np.mean(fpr), DIGITS)])\n",
    "\n",
    "    f.close()\n",
    "\n",
    "    # f = csv_file_io.open_csv(RESULT_FILE_DIR + file_name + now_date_time + '.csv', 'a')\n",
    "    # csv_file_io.write_csv_data(f, [kernel, acc, tpr, fnr, tnr, fpr])\n",
    "    # f.close()\n",
    "\n",
    "\n",
    "def save_total_classification_performance(res_dict: dict):\n",
    "    file_name = FILE_NAME.split(')')[0] + 'total) '\n",
    "    f = csv_file_io.open_csv(RESULT_FILE_DIR + file_name + now_date_time + '.csv', 'w')\n",
    "    csv_file_io.write_csv_data(f, ['Kernel', 'ACC', 'TPR', 'FNR', 'TNR', 'FPR'])\n",
    "    # f.close()\n",
    "\n",
    "    for i in range(0, len(res_dict['kernel'])):\n",
    "        # f = csv_file_io.open_csv(RESULT_FILE_DIR + FILE_NAME + now_date_time + '.csv', 'a')\n",
    "        csv_file_io.write_csv_data(f, [res_dict['kernel'][i], res_dict['acc'][i],  res_dict['tpr'][i], res_dict['fnr'][i], res_dict['tnr'][i], res_dict['fpr'][i]])    \n",
    "\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_integrated_train_validation_signal = preprocessor.get_integrated_signal(TRAINING_VALIDATION_DATASET_DIR, type='TRAINING+VALIDATION')\n",
    "dict_integrated_test_signal = preprocessor.get_integrated_signal(TEST_DATASET_DIR, type='TEST')\n",
    "\n",
    "# Visualizing Audio (waveform and power spectrum)\n",
    "if b_visualizing == True:\n",
    "    for key, val in dict_integrated_train_validation_signal.items():\n",
    "        display.display_waveshow(val, key + \"'s Training+Validation Signal's Waveform\")\n",
    "        display.display_power_spectrum(val, key + \"'s Training+Validation Signal's Power Spectrum\")\n",
    "\n",
    "    for key, val in dict_integrated_test_signal.items():\n",
    "        display.display_waveshow(val, key + \"'s Test Signal's Waveform\")\n",
    "        display.display_power_spectrum(val, key + \"'s Test Signal's Power Spectrum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction (MFCC)\n",
    "N_MFCC = 40\n",
    "n_fft = int(SAMPLE_RATE * FRAME_LENGTH) # length of the FFT window, frame 하나당 sample의 수 / n_fft=2048, n_fft = int(SAMPLE_RATE * FRAME_LENGTH)\n",
    "hop_length = int(SAMPLE_RATE * FRAME_STRIDE) # number of samples between successive frames / hop_length=512, hop_length = int(SAMPLE_RATE * FRAME_STRIDE)\n",
    "n_mels = 128\n",
    "f_max = 8000\n",
    "\n",
    "dict_melspectrogram_train_validation_signal = {}\n",
    "for key, val in dict_integrated_train_validation_signal.items():\n",
    "    dict_melspectrogram_train_validation_signal[key] = librosa.feature.melspectrogram(val, sr=SAMPLE_RATE, n_mels=n_mels, fmax=f_max, n_fft=n_fft, hop_length=hop_length)\n",
    "\n",
    "dict_melspectrogram_test_signal = {}\n",
    "for key, val in dict_integrated_test_signal.items():\n",
    "    dict_melspectrogram_test_signal[key] = librosa.feature.melspectrogram(val, sr=SAMPLE_RATE, n_mels=n_mels, fmax=f_max, n_fft=n_fft, hop_length=hop_length)\n",
    "\n",
    "dict_mfccs_train_validation_signal = {}\n",
    "for key, val in dict_melspectrogram_train_validation_signal.items():\n",
    "    dict_mfccs_train_validation_signal[key] = librosa.feature.mfcc(S=librosa.power_to_db(val), sr=SAMPLE_RATE, n_mfcc=N_MFCC)\n",
    "\n",
    "dict_mfccs_test_signal = {}\n",
    "for key, val in dict_melspectrogram_test_signal.items():\n",
    "    dict_mfccs_test_signal[key] = librosa.feature.mfcc(S=librosa.power_to_db(val), sr=SAMPLE_RATE, n_mfcc=N_MFCC)\n",
    "\n",
    "if b_visualizing == True:\n",
    "    for key, val in dict_mfccs_train_validation_signal.items():\n",
    "        display.display_mfccs(val,  \"'s Training+Validation Signal's MFCCs\")\n",
    "    \n",
    "    for key, val in dict_mfccs_test_signal.items():\n",
    "        display.display_mfccs(val,  \"'s TEST Signal's MFCCs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_train_validation_data = None\n",
    "arr_train_validation_target = None\n",
    "\n",
    "for key, val in dict_mfccs_train_validation_signal.items():    \n",
    "    arr_mfccs_data = val.transpose()    \n",
    "    arr_target = np.full((arr_mfccs_data.shape[0], 1), key)   \n",
    "\n",
    "    if arr_train_validation_data is None:\n",
    "        arr_train_validation_data = arr_mfccs_data\n",
    "        arr_train_validation_target = arr_target        \n",
    "    else:       \n",
    "        arr_train_validation_data = np.vstack((arr_train_validation_data, arr_mfccs_data))\n",
    "        arr_train_validation_target = np.vstack((arr_train_validation_target, arr_target))\n",
    "\n",
    "arr_test_data = None\n",
    "arr_test_target = None\n",
    "\n",
    "for key, val in dict_mfccs_test_signal.items():    \n",
    "    arr_mfccs_data = val.transpose()    \n",
    "    arr_target = np.full((arr_mfccs_data.shape[0], 1), key)   \n",
    "\n",
    "    if arr_test_data is None:\n",
    "        arr_test_data = arr_mfccs_data\n",
    "        arr_test_target = arr_target        \n",
    "    else:       \n",
    "        arr_test_data = np.vstack((arr_test_data, arr_mfccs_data))\n",
    "        arr_test_target = np.vstack((arr_test_target, arr_target))\n",
    "\n",
    "print(\"Training+Validation data / target : \", arr_train_validation_data.shape, arr_train_validation_target.shape)\n",
    "print(\"Test data / target : \", arr_test_data.shape, arr_test_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_std_train_validation_data, arr_std_test_tata = classifier.set_scaler_train_test(arr_train_validation_data, arr_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_scores = {'kernel': [], 'acc': [], 'tpr': [], 'fnr': [], 'tnr': [], 'fpr': []}\n",
    "targets_name = preprocessor.get_target_names(TEST_DATASET_DIR)\n",
    "\n",
    "# linear\n",
    "svm_model = classifier.set_fit_svm_model(\"linear\", arr_std_train_validation_data, arr_train_validation_target)\n",
    "acc, tpr, fnr, tnr, fpr = classifier.predict_svm_classification(svm_model, arr_std_test_tata, arr_test_target, targets_name, now_date_time)\n",
    "save_each_classification_performance(svm_model.kernel, targets_name, acc, tpr, fnr, tnr, fpr)\n",
    "\n",
    "dict_scores['kernel'].append(svm_model.kernel)        \n",
    "dict_scores['acc'].append(acc)\n",
    "dict_scores['tpr'].append(round(np.mean(tpr), DIGITS))\n",
    "dict_scores['fnr'].append(round(np.mean(fnr), DIGITS))\n",
    "dict_scores['tnr'].append(round(np.mean(tnr), DIGITS))\n",
    "dict_scores['fpr'].append(round(np.mean(fpr), DIGITS))\n",
    "\n",
    "# LinearSVC\n",
    "svm_model = classifier.set_fit_svm_model(\"LinearSVC\", arr_std_train_validation_data, arr_train_validation_target)\n",
    "acc, tpr, fnr, tnr, fpr = classifier.predict_svm_classification(svm_model, arr_std_test_tata, arr_test_target, targets_name, now_date_time)\n",
    "\n",
    "save_each_classification_performance(\"LinearSVC\", targets_name, acc, tpr, fnr, tnr, fpr)\n",
    "\n",
    "dict_scores['kernel'].append(\"LinearSVC\")        \n",
    "dict_scores['acc'].append(acc)\n",
    "dict_scores['tpr'].append(round(np.mean(tpr), DIGITS))\n",
    "dict_scores['fnr'].append(round(np.mean(fnr), DIGITS))\n",
    "dict_scores['tnr'].append(round(np.mean(tnr), DIGITS))\n",
    "dict_scores['fpr'].append(round(np.mean(fpr), DIGITS))\n",
    "\n",
    "# rbf\n",
    "svm_model = classifier.set_fit_svm_model(\"rbf\", arr_std_train_validation_data, arr_train_validation_target)\n",
    "acc, tpr, fnr, tnr, fpr = classifier.predict_svm_classification(svm_model, arr_std_test_tata, arr_test_target, targets_name, now_date_time)\n",
    "\n",
    "save_each_classification_performance(svm_model.kernel, targets_name, acc, tpr, fnr, tnr, fpr)\n",
    "\n",
    "dict_scores['kernel'].append(svm_model.kernel)        \n",
    "dict_scores['acc'].append(acc)\n",
    "dict_scores['tpr'].append(round(np.mean(tpr), DIGITS))\n",
    "dict_scores['fnr'].append(round(np.mean(fnr), DIGITS))\n",
    "dict_scores['tnr'].append(round(np.mean(tnr), DIGITS))\n",
    "dict_scores['fpr'].append(round(np.mean(fpr), DIGITS))\n",
    "\n",
    "# poly\n",
    "svm_model = classifier.set_fit_svm_model(\"poly\", arr_std_train_validation_data, arr_train_validation_target)\n",
    "acc, tpr, fnr, tnr, fpr = classifier.predict_svm_classification(svm_model, arr_std_test_tata, arr_test_target, targets_name, now_date_time)\n",
    "\n",
    "save_each_classification_performance(svm_model.kernel, targets_name, acc, tpr, fnr, tnr, fpr)\n",
    "\n",
    "dict_scores['kernel'].append(svm_model.kernel)        \n",
    "dict_scores['acc'].append(acc)\n",
    "dict_scores['tpr'].append(round(np.mean(tpr), DIGITS))\n",
    "dict_scores['fnr'].append(round(np.mean(fnr), DIGITS))\n",
    "dict_scores['tnr'].append(round(np.mean(tnr), DIGITS))\n",
    "dict_scores['fpr'].append(round(np.mean(fpr), DIGITS))\n",
    "\n",
    "# sigmoid\n",
    "svm_model = classifier.set_fit_svm_model(\"sigmoid\", arr_std_train_validation_data, arr_train_validation_target)\n",
    "acc, tpr, fnr, tnr, fpr = classifier.predict_svm_classification(svm_model, arr_std_test_tata, arr_test_target, targets_name, now_date_time)\n",
    "\n",
    "save_each_classification_performance(svm_model.kernel, targets_name, acc, tpr, fnr, tnr, fpr)\n",
    "\n",
    "dict_scores['kernel'].append(svm_model.kernel)        \n",
    "dict_scores['acc'].append(acc)\n",
    "dict_scores['tpr'].append(round(np.mean(tpr), DIGITS))\n",
    "dict_scores['fnr'].append(round(np.mean(fnr), DIGITS))\n",
    "dict_scores['tnr'].append(round(np.mean(tnr), DIGITS))\n",
    "dict_scores['fpr'].append(round(np.mean(fpr), DIGITS))\n",
    "\n",
    "save_total_classification_performance(dict_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier.cross_validation_svm(\"linear\", x_train_std, y_train)\n",
    "# classifier.gridsearchCV_svm(\"linear\", x_train_std, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
